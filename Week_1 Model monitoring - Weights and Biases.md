# Â ML Model Monitoring

Why should you monitor your model? There are many reasons. It can help you understand the accuracy of your predictions, prevent prediction errors, and tweak your models to perfect them.

Generally we will be running experiments by tweaking hyper-parameters, trying different models to test their performance, see the connection between your model and the input data, and perform advanced tests. Having all these logged at a single place will help in getting better and faster insights.

The easiest way to ensure things work smoothly is to use ML model monitoring tools.

Dedicated tools can also be used to collaborate with your team, share your work with other peopleâ€”itâ€™s a shared space for teams to collaborate, participate in model creation and further monitoring. Itâ€™s easier to exchange ideas, thoughts and observations, and spot errors when you have real-time insight into whatâ€™s happening with your models.

There are many libraries available to monitor machine learning models. The prominent ones are:

- [`Comet`](https://www.comet.ml/site/)
    
- [`MLFlow`](https://mlflow.org/)
    
- [`Neptune`](https://neptune.ai/)
    
- [`TensorBoard`](https://www.tensorflow.org/tensorboard)
    
- [`Weights and Bias`](https://wandb.ai/site)
    

and many more...

I will be usingÂ `Weights and Bias`.

In this post, I will be going through the following topics:

- `How to configure basic logging with W&B?`
- `How to compute metrics and log them in W&B?`
- `How to add plots in W&B?`
- `How to add data samples to W&B?`

_Note: Basic knowledge of Machine Learning, Pytorch Lightning is needed_

## ğŸ‹ï¸ Weights and Bias Configuration

In order to use W&B, an account needs to be created. (Free for public projects and 100GB storage). Once account is created, we need to login.

Run the command:

```shell
wandb login
```

You will be prompted with the following:

![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjMyNiIgaGVpZ2h0PSIxMTQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+)

![wandb](https://deep-learning-blogs.vercel.app/_next/image?url=%2Fstatic%2Fimages%2Fwandb%2Fwandb.png&w=3840&q=75)

Follow the authorisation link:Â [https://wandb.ai/authorize](https://wandb.ai/authorize)Â and copy paste the api key.

## Configuring ğŸ‹ï¸ ğŸ¤ âš¡ï¸

Create a project atÂ `W&B`Â and then use the same name here. So that all the experiments will be logged into that project.

```python
from pytorch_lightning.loggers import WandbLogger
wandb_logger = WandbLogger(project="MLOps Basics")
```

Now pass this as theÂ `logger`Â to theÂ `Trainer`.

```python
trainer = pl.Trainer(
        max_epochs=3,
        logger=wandb_logger,
        callbacks=[checkpoint_callback],
    )
```

Now all the logs will be tracked in W&B.

## ğŸ“ˆ Metrics

Metrics calculation can sometimes become daunting. Fortunately pytorch lightning team has been building a libraryÂ `torchmetrics`Â which contains all the prominent metrics. Check theÂ [documentation](https://torchmetrics.readthedocs.io/en/latest/)Â for more information.

Since the problem is about classification, Let's see how to calculate metrics likeÂ `Accuracy`,Â `Precision`,Â `Recall`,Â `F1`.

Let's import theÂ `torchmetrics`Â library as

```python
import torchmetrics
```

Then declare the metrics inÂ `__init__`

```python
class ColaModel(pl.LightningModule):
    def __init__(self, model_name="google/bert_uncased_L-2_H-128_A-2", lr=3e-5):
        self.train_accuracy_metric = torchmetrics.Accuracy()
        self.val_accuracy_metric = torchmetrics.Accuracy()
        self.f1_metric = torchmetrics.F1(num_classes=self.num_classes)
        self.precision_macro_metric = torchmetrics.Precision(
            average="macro", num_classes=self.num_classes
        )
        self.recall_macro_metric = torchmetrics.Recall(
            average="macro", num_classes=self.num_classes
        )
        self.precision_micro_metric = torchmetrics.Precision(average="micro")
        self.recall_micro_metric = torchmetrics.Recall(average="micro")
```

Metrics can be calculated at different steps like duringÂ `training`,Â `validation`Â andÂ `testing`.

Pytorch Lightning Module âš¡ï¸ comes with different methods which makes our job easy on where to implement the metrics calculation.

The two main methods where the metrics usually calculated are:

- `training_step`: This is where a batch of training data is processed. Metrics likeÂ `training loss`,Â `training_accuracy`Â can be computed here.
- `validation_step`: This is where a batch of validation data is processed. Metrics likeÂ `validation_loss`,Â `validation_accuracy`Â etc can be computed here.

There are other methods also available:

- `training_epoch_end`: This is called at the end of every training epoch. All the data which is returned byÂ `training_step`Â can be aggregated here.
- `validation_epoch_end`: This is called at the end of every training epoch. All the data which is returned byÂ `training_step`Â can be aggregated here.
- `test_step`: This is called when trainer is called with test method i.eÂ `trainer.test()`.
- `test_epoch_end`: This is called at the end of all test batches.

Few configurations available for logging:

- SettingÂ `prog_bar=True`Â which will enable to show metrics on the progress bar.
- SettingÂ `on_epoch=True`, the metrics will be aggregated and averaged across the batches in an epoch.
- SettingÂ `on_step=True`, the metrics will be logged for each batch. (useful for loss)

By default:

- Logging inÂ `training_step`Â hasÂ `on_step=True`
- Logging inÂ `validation_step`Â hasÂ `on_step=False`,Â `on_epoch=True`

For more, refer to theÂ [documentation here](https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#automatic-logging)

Now let's see how metrics calculation and logging looks like:

```python
def training_step(self, batch, batch_idx):
    outputs = self.forward(
        batch["input_ids"], batch["attention_mask"], labels=batch["label"]
    )
    # loss = F.cross_entropy(logits, batch["label"])
    preds = torch.argmax(outputs.logits, 1)
    train_acc = self.train_accuracy_metric(preds, batch["label"])
    self.log("train/loss", outputs.loss, prog_bar=True, on_epoch=True)
    self.log("train/acc", train_acc, prog_bar=True, on_epoch=True)
    return outputs.loss
```

SinceÂ `on_epoch=True`Â is enabled, the plots in W&B ğŸ‹ï¸ will haveÂ `train/loss_step`,Â `train/loss_epoch`Â andÂ `train/acc_step`,Â `train/acc_epoch`.

![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjI3OCIgaGVpZ2h0PSI2MzQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+)

![train_loss](https://deep-learning-blogs.vercel.app/_next/image?url=%2Fstatic%2Fimages%2Fwandb%2Ftrain_1.png&w=3840&q=75)

![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjI1NiIgaGVpZ2h0PSI2MzYiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+)

![train_acc](https://deep-learning-blogs.vercel.app/_next/image?url=%2Fstatic%2Fimages%2Fwandb%2Ftrain_2.png&w=3840&q=75)

During validation, we might want to monitor more metrics likeÂ `Precision, Recall, F1`.

```python
def validation_step(self, batch, batch_idx):
    labels = batch["label"]
    outputs = self.forward(
        batch["input_ids"], batch["attention_mask"], labels=batch["label"]
    )
    preds = torch.argmax(outputs.logits, 1)

    # Metrics
    valid_acc = self.val_accuracy_metric(preds, labels)
    precision_macro = self.precision_macro_metric(preds, labels)
    recall_macro = self.recall_macro_metric(preds, labels)
    precision_micro = self.precision_micro_metric(preds, labels)
    recall_micro = self.recall_micro_metric(preds, labels)
    f1 = self.f1_metric(preds, labels)

    # Logging metrics
    self.log("valid/loss", outputs.loss, prog_bar=True, on_step=True)
    self.log("valid/acc", valid_acc, prog_bar=True)
    self.log("valid/precision_macro", precision_macro, prog_bar=True)
    self.log("valid/recall_macro", recall_macro, prog_bar=True)
    self.log("valid/precision_micro", precision_micro, prog_bar=True)
    self.log("valid/recall_micro", recall_micro, prog_bar=True)
    self.log("valid/f1", f1, prog_bar=True)
    return {"labels": labels, "logits": outputs.logits}
```

![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMzM4MCIgaGVpZ2h0PSIxMjY2IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIvPg==)

![valid](https://deep-learning-blogs.vercel.app/_next/image?url=%2Fstatic%2Fimages%2Fwandb%2Fvalid.png&w=3840&q=75)

The values returned during theÂ `validation_step`Â can be aggregated in theÂ `validation_epoch_end`Â and any transformations can be done using that.

For example, as shown in the above code snippetÂ `labels, logits`Â are returned.

These values can be aggregated in theÂ `validation_epoch_end`Â method and metric likeÂ `confusion matrix`Â can be computed.

```python
def validation_epoch_end(self, outputs):
    labels = torch.cat([x["labels"] for x in outputs])
    logits = torch.cat([x["logits"] for x in outputs])
    preds = torch.argmax(logits, 1)

    cm = confusion_matrix(labels.numpy(), preds.numpy())
```

## ğŸ“‰ Adding Plots to ğŸ‹ï¸

Logging metrics might not be sufficient every time. Having more visual information like graphs and plots will help in understanding the model performance better.

There are multiple ways to plot graphs in ğŸ‹ï¸. Let's see a couple of ways.

As an example, let's see how to plotÂ `confusion_matrix`Â computed above.

### Method 1

ğŸ‹ï¸ has built-inÂ `wandb.plot`Â methods (**preferrable**Â since it offers lot of customizations). Check for all available methods here:Â [documentation](https://docs.wandb.ai/guides/track/log#custom-charts)

PlottingÂ `confusion matrix`Â looks like:

```python
# 1. Confusion matrix plotting using inbuilt W&B method
self.logger.experiment.log(
    {
        "conf": wandb.plot.confusion_matrix(
            probs=logits.numpy(), y_true=labels.numpy()
        )
    }
)
```

The plot looks like:

![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTE0NiIgaGVpZ2h0PSI2NDAiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+)

![cm1](https://deep-learning-blogs.vercel.app/_next/image?url=%2Fstatic%2Fimages%2Fwandb%2Fcm_1.png&w=3840&q=75)

### Method 2

ğŸ‹ï¸ supportsÂ `scikit-learn`Â integration also. Which means whatever the plots available inÂ `scikit-learn`Â can be plotted in ğŸ‹ï¸. Refer to theÂ [documentation](https://docs.wandb.ai/guides/integrations/scikit)Â for more details.

Plotting ofÂ `confusion matrix`Â using scikit-learn looks like:

```python
# 2. Confusion Matrix plotting using scikit-learn method
wandb.log({"cm": wandb.sklearn.plot_confusion_matrix(labels.numpy(), preds)})
```

The plot looks like:

![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMzYxMCIgaGVpZ2h0PSI2MzIiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+)

![cm2](https://deep-learning-blogs.vercel.app/_next/image?url=%2Fstatic%2Fimages%2Fwandb%2Fcm_2.png&w=3840&q=75)

### Method 3

ğŸ‹ï¸ supports plotting librariesÂ `matplotlib`,Â `plotly`Â etc. Refer to theÂ [documentation](https://docs.wandb.ai/guides/track/log#matplotlib)Â for more details.

This means we can create our own plot and log them in ğŸ‹ï¸

```python
# 3. Confusion Matric plotting using Seaborn
data = confusion_matrix(labels.numpy(), preds.numpy())
df_cm = pd.DataFrame(data, columns=np.unique(labels), index=np.unique(labels))
df_cm.index.name = "Actual"
df_cm.columns.name = "Predicted"
plt.figure(figsize=(10, 5))
plot = sns.heatmap(
    df_cm, cmap="Blues", annot=True, annot_kws={"size": 16}
)  # font size
self.logger.experiment.log({"Confusion Matrix": wandb.Image(plot)})
```

The plot looks like:
![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQxMiIgaGVpZ2h0PSI4NDYiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+)
![cm3](https://deep-learning-blogs.vercel.app/_next/image?url=%2Fstatic%2Fimages%2Fwandb%2Fcm_3.png&w=3840&q=75)

Now that we know how to add graphs in ğŸ‹ï¸ , let's see how to add data samples (images, text etc) to ğŸ‹ï¸

## ğŸ“ Adding Data samples to ğŸ‹ï¸

Once the model is trained, we need to understand where the model is performing well and where it is not.

Since we are working onÂ `cola`Â problem, let's look at few samples where the model is not performing good and log it to ğŸ‹ï¸

There can be a lot of ways to plot the data. Refer toÂ [documentation](https://docs.wandb.ai/guides/data-vis/tables-quickstart#1-log-a-table)Â here for more details.

This can be achieved viaÂ `callback`Â ğŸ” mechanism in âš¡ï¸

```python
class SamplesVisualisationLogger(pl.Callback):
    def __init__(self, datamodule):
        super().__init__()

        self.datamodule = datamodule

    def on_validation_end(self, trainer, pl_module):
        # can be done on complete dataset also
        val_batch = next(iter(self.datamodule.val_dataloader()))
        sentences = val_batch["sentence"]

        # get the predictions
        outputs = pl_module(val_batch["input_ids"], val_batch["attention_mask"])
        preds = torch.argmax(outputs.logits, 1)
        labels = val_batch["label"]

        # predicted and labelled data
        df = pd.DataFrame(
            {"Sentence": sentences, "Label": labels.numpy(), "Predicted": preds.numpy()}
        )

        # wrongly predicted data
        wrong_df = df[df["Label"] != df["Predicted"]]

        # Logging wrongly predicted dataframe as a table
        trainer.logger.experiment.log(
            {
                "examples": wandb.Table(dataframe=wrong_df, allow_mixed_types=True),
                "global_step": trainer.global_step,
            }
        )
```

Then add this callback ğŸ” to trainer ğŸ‘Ÿ

```python
trainer = pl.Trainer(
        max_epochs=3,
        logger=wandb_logger,
        callbacks=[checkpoint_callback, SamplesVisualisationLogger(cola_data)],
        log_every_n_steps=10,
        deterministic=True,
    )
```

In ğŸ‹ï¸ this will look like

![](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMzM5NCIgaGVpZ2h0PSIxNTY2IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIvPg==)

![samples](https://deep-learning-blogs.vercel.app/_next/image?url=%2Fstatic%2Fimages%2Fwandb%2Fsamples.png&w=3840&q=75)

## ğŸ”š

This conculdes the post. In the next post, I will be going through:

- `How to do configuration using Hydra?`

Complete code for this post can also be found here:Â [Github](https://github.com/graviraja/MLOps-Basics)

## References

- [Tutorial on Pytorch Lightning + Weights & Bias](https://www.youtube.com/watch?v=hUXQm46TAKc)
    
- [WandB Documentation](https://docs.wandb.ai/)